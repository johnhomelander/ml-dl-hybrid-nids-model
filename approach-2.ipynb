{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3eb369-2024-4295-881f-34c857a3adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Step 0: Import Required Libraries\n",
    "# =============================\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0897bf7-55f7-4458-bd93-1383aaf63c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Step 1: Data Ingestion - Combine Multiple CSV Files (if applicable)\n",
    "# =============================\n",
    "# Set the directory containing your CSV files (change the path as needed)\n",
    "data_dir = \"path/to/cicids_csvs\"  # Update with your directory path\n",
    "\n",
    "# Use glob to create a list of CSV file paths\n",
    "csv_files = glob.glob(os.path.join(data_dir, \"*.csv\"))\n",
    "\n",
    "# Read and combine CSV files into one DataFrame\n",
    "df_list = []\n",
    "for file in csv_files:\n",
    "    temp_df = pd.read_csv(file)\n",
    "    # Optional: add a column to indicate the source file or attack type, if needed\n",
    "    df_list.append(temp_df)\n",
    "\n",
    "# If you have multiple CSVs, concatenate them; otherwise, read a single CSV.\n",
    "if len(df_list) > 1:\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "else:\n",
    "    df = df_list[0]\n",
    "\n",
    "print(\"Combined dataset shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6c7e82-5d49-48a8-826a-a28f466e7fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Step 2: Data Preprocessing\n",
    "# =============================\n",
    "\n",
    "# 2.1 Drop Irrelevant Columns (adjust as per your dataset)\n",
    "columns_to_drop = ['Flow ID', 'Source IP', 'Destination IP', 'Timestamp']\n",
    "df.drop(columns=columns_to_drop, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "# 2.2 Handle Missing Values\n",
    "# Fill missing values for numeric columns only\n",
    "numeric_medians = df.select_dtypes(include=[np.number]).median()\n",
    "df.fillna(numeric_medians, inplace=True)\n",
    "\n",
    "# 2.3 Encode the Label Column\n",
    "# Ensure the label column name is correct (here assumed to be 'Label')\n",
    "label_encoder = LabelEncoder()\n",
    "df['Label'] = label_encoder.fit_transform(df['Label'])\n",
    "\n",
    "# 2.4 Separate Features and Labels\n",
    "X = df.drop('Label', axis=1)\n",
    "y = df['Label']\n",
    "\n",
    "# 2.5 Feature Scaling on Raw Features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 2.6 Balance the Dataset using SMOTE (if needed)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n",
    "\n",
    "# 2.7 Split Data into Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, \n",
    "                                                    test_size=0.2, random_state=42)\n",
    "print(\"Preprocessing complete. Training samples:\", X_train.shape[0], \n",
    "      \"Test samples:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b25bf0-0b30-4d83-aa0d-20aee7712ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Step 3: DL Model for Feature Extraction using Autoencoder\n",
    "# =============================\n",
    "\n",
    "# Define Autoencoder architecture\n",
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 32  # Dimension of the latent (encoded) space\n",
    "\n",
    "# Build the autoencoder model\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "# Encoder part\n",
    "encoded = Dense(64, activation='relu')(input_layer)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "# Decoder part\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "print(\"Training Autoencoder for Feature Extraction...\")\n",
    "autoencoder.fit(X_train, X_train, \n",
    "                epochs=20, \n",
    "                batch_size=32, \n",
    "                validation_split=0.1, \n",
    "                verbose=1)\n",
    "\n",
    "# Extract encoder model to generate latent features\n",
    "encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "\n",
    "# Generate latent (encoded) features for training and testing data\n",
    "X_train_encoded = encoder.predict(X_train)\n",
    "X_test_encoded = encoder.predict(X_test)\n",
    "\n",
    "print(\"Encoded training features shape:\", X_train_encoded.shape)\n",
    "print(\"Encoded testing features shape:\", X_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3d2538-de7b-4edc-b60e-a124a8417bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Step 4: ML Classifier on Encoded Features\n",
    "# =============================\n",
    "\n",
    "# Train a Random Forest classifier using the latent features\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_rf = rf_clf.predict(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a1b914-bcbc-421a-974f-ca12deeae614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Step 5: Evaluation\n",
    "# =============================\n",
    "\n",
    "print(\"\\nRandom Forest Classification Report on Encoded Features:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b493082-c322-4a08-8091-e81b4d4f6ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5edefd-f669-494c-af8d-d584ad65b1df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd964b2-3841-4069-beb3-5fcc2d5e24e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
